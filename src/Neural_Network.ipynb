{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, threshold = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    resized = cv2.resize(threshold, (28, 28))\n",
    "    normalized = resized / 255.0\n",
    "    reshaped = np.reshape(normalized, (28, 28, 1))\n",
    "    return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.listdir('../img_out/')\n",
    "name_list = []\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 4\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "image_data = []\n",
    "\n",
    "for img in img_path:\n",
    "    name_list.append(img.split(\".\")[0])\n",
    "\n",
    "    # Carregar a imagem de entrada\n",
    "    image = cv2.imread(f'../img_out/{img}')\n",
    "\n",
    "    # Pr√©-processar a imagem\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "\n",
    "    image_data.append(preprocessed_image)\n",
    "\n",
    "data = pd.read_csv(r\"../SimpleEQ.csv\",quotechar=\"'\",sep=\";\",names=[\"latex\",\"opr\",\"img_name\"])\n",
    "\n",
    "df_filtered = data.loc[data['img_name'].isin(name_list)]\n",
    "\n",
    "x = np.array(image_data)\n",
    "y = np.array(pd.get_dummies(df_filtered[\"opr\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (91265, 28, 28, 1, 1)\n",
      "91265 train samples\n",
      "10140 test samples\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 13, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 6404      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,220\n",
      "Trainable params: 25,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "642/642 [==============================] - 117s 89ms/step - loss: 1.3864 - accuracy: 0.2493 - val_loss: 1.3863 - val_accuracy: 0.2507\n",
      "Epoch 2/15\n",
      "642/642 [==============================] - 54s 84ms/step - loss: 1.3864 - accuracy: 0.2481 - val_loss: 1.3863 - val_accuracy: 0.2507\n",
      "Epoch 3/15\n",
      "642/642 [==============================] - 55s 86ms/step - loss: 1.3864 - accuracy: 0.2487 - val_loss: 1.3863 - val_accuracy: 0.2502\n",
      "Epoch 4/15\n",
      "642/642 [==============================] - 54s 84ms/step - loss: 1.3864 - accuracy: 0.2473 - val_loss: 1.3863 - val_accuracy: 0.2507\n",
      "Epoch 5/15\n",
      "642/642 [==============================] - 54s 84ms/step - loss: 1.3863 - accuracy: 0.2494 - val_loss: 1.3863 - val_accuracy: 0.2484\n",
      "Epoch 6/15\n",
      "642/642 [==============================] - 56s 87ms/step - loss: 1.3863 - accuracy: 0.2484 - val_loss: 1.3863 - val_accuracy: 0.2507\n",
      "Epoch 7/15\n",
      "642/642 [==============================] - 53s 83ms/step - loss: 1.3863 - accuracy: 0.2494 - val_loss: 1.3863 - val_accuracy: 0.2484\n",
      "Epoch 8/15\n",
      "642/642 [==============================] - 53s 83ms/step - loss: 1.3863 - accuracy: 0.2497 - val_loss: 1.3863 - val_accuracy: 0.2507\n",
      "Epoch 9/15\n",
      "642/642 [==============================] - 53s 83ms/step - loss: 1.3864 - accuracy: 0.2486 - val_loss: 1.3863 - val_accuracy: 0.2484\n",
      "Epoch 10/15\n",
      "642/642 [==============================] - 54s 84ms/step - loss: 1.3863 - accuracy: 0.2492 - val_loss: 1.3864 - val_accuracy: 0.2484\n",
      "Epoch 11/15\n",
      "642/642 [==============================] - 55s 85ms/step - loss: 1.3863 - accuracy: 0.2479 - val_loss: 1.3863 - val_accuracy: 0.2502\n",
      "Epoch 12/15\n",
      "642/642 [==============================] - 54s 85ms/step - loss: 1.3863 - accuracy: 0.2474 - val_loss: 1.3863 - val_accuracy: 0.2507\n",
      "Epoch 13/15\n",
      "642/642 [==============================] - 55s 86ms/step - loss: 1.3863 - accuracy: 0.2505 - val_loss: 1.3863 - val_accuracy: 0.2502\n",
      "Epoch 14/15\n",
      "642/642 [==============================] - 54s 83ms/step - loss: 1.3864 - accuracy: 0.2491 - val_loss: 1.3863 - val_accuracy: 0.2507\n",
      "Epoch 15/15\n",
      "642/642 [==============================] - 54s 84ms/step - loss: 1.3863 - accuracy: 0.2488 - val_loss: 1.3863 - val_accuracy: 0.2507\n",
      "Test loss: 1.3863145112991333\n",
      "Test accuracy: 0.2495069056749344\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.1\n",
    "x_train = x[0:int(len(x)*(1-test_size))]\n",
    "x_test = x[int(len(x)*(1-test_size)+1):]\n",
    "\n",
    "y_train = y[0:int(len(y)*(1-test_size))]\n",
    "y_test = y[int(len(y)*(1-test_size)+1):]\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.listdir('../img_out/')\n",
    "name_list = []\n",
    "\n",
    "for img in img_path:\n",
    "    name_list.append(img.split(\".\")[0])\n",
    "\n",
    "data = pd.read_csv(r\"../SimpleEQ.csv\",quotechar=\"'\",sep=\";\",names=[\"latex\",\"opr\",\"img_name\"])\n",
    "\n",
    "df_filtered = data.loc[data['img_name'].isin(name_list)]\n",
    "data.loc[~data['img_name'].isin(df_filtered['img_name'])].sort_index(ascending=False).to_csv(r\"../RemainEQ.csv\",sep=\";\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 0.3\n",
    "x_train = x[0:int(len(x)*(1-test_size))]\n",
    "x_test = x[int(len(x)*(1-test_size)+1):]\n",
    "\n",
    "y_train = y[0:int(len(y)*(1-test_size))]\n",
    "y_test = y[int(len(y)*(1-test_size)+1):]\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14585, 28, 28, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(489720, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"../SimpleEQ.csv\",quotechar=\"'\",sep=\";\",names=[\"latex\",\"opr\",\"img_name\"])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48620, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = data.loc[data['img_name'].isin(name_list)]\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48620, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_img_data = np.array(image_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np_img_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(x_train\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(x_train))\n\u001b[1;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(np_img_data\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(np_img_data))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np_img_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(type(x_train))\n",
    "print(np_img_data.shape)\n",
    "print(type(np_img_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n",
    "lista = [n for n in range(4)]\n",
    "arr = np.array(lista)\n",
    "np.reshape(arr,(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "422/422 [==============================] - 42s 95ms/step - loss: 0.3644 - accuracy: 0.8869 - val_loss: 0.0819 - val_accuracy: 0.9783\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 33s 78ms/step - loss: 0.1140 - accuracy: 0.9660 - val_loss: 0.0601 - val_accuracy: 0.9837\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 33s 78ms/step - loss: 0.0870 - accuracy: 0.9731 - val_loss: 0.0478 - val_accuracy: 0.9872\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 32s 76ms/step - loss: 0.0703 - accuracy: 0.9783 - val_loss: 0.0440 - val_accuracy: 0.9863\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 32s 76ms/step - loss: 0.0639 - accuracy: 0.9806 - val_loss: 0.0378 - val_accuracy: 0.9900\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 32s 76ms/step - loss: 0.0563 - accuracy: 0.9827 - val_loss: 0.0337 - val_accuracy: 0.9922\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 32s 76ms/step - loss: 0.0526 - accuracy: 0.9841 - val_loss: 0.0333 - val_accuracy: 0.9912\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 32s 76ms/step - loss: 0.0479 - accuracy: 0.9846 - val_loss: 0.0326 - val_accuracy: 0.9915\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 33s 79ms/step - loss: 0.0448 - accuracy: 0.9856 - val_loss: 0.0334 - val_accuracy: 0.9905\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 33s 79ms/step - loss: 0.0411 - accuracy: 0.9871 - val_loss: 0.0305 - val_accuracy: 0.9925\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 32s 75ms/step - loss: 0.0399 - accuracy: 0.9874 - val_loss: 0.0305 - val_accuracy: 0.9912\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 32s 75ms/step - loss: 0.0376 - accuracy: 0.9877 - val_loss: 0.0288 - val_accuracy: 0.9925\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 32s 75ms/step - loss: 0.0352 - accuracy: 0.9889 - val_loss: 0.0302 - val_accuracy: 0.9922\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 32s 75ms/step - loss: 0.0338 - accuracy: 0.9885 - val_loss: 0.0298 - val_accuracy: 0.9930\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.0323 - accuracy: 0.9894 - val_loss: 0.0268 - val_accuracy: 0.9918\n",
      "Test loss: 0.02883036434650421\n",
      "Test accuracy: 0.9908000230789185\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "# print(np.array(image_data).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "os.add_dll_directory(r'c:\\users\\luiz souza\\anaconda3\\envs\\latex\\lib\\site-packages')\n",
    "import pytesseract\n",
    "import sympy\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('teste.csv',sep=';',quotechar=\"'\")\n",
    "df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,v in zip([\"a\",\"B\",\"x\"],[1,2,3]):\n",
    "    print(str(f\"'{n}{v}+{n}';soma;soma_numero\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tex2fig(serie:pd.Series):\n",
    "    latex = serie[0]\n",
    "    img_name = serie[2]\n",
    "\n",
    "    plt.figure(dpi=25)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    left, width = .25, .5\n",
    "    bottom, height = .25, .5\n",
    "    right = left + width\n",
    "    top = bottom + height\n",
    "\n",
    "    # Fractions and radicals\n",
    "    ax.text(0.5 * (left + right), 0.5 * (bottom + top), rf\"${latex}$\",\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='center',\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=150)\n",
    "\n",
    "    #Adjusting the ticks size of the axes\n",
    "    plt.xlim(0,2)\n",
    "    plt.ylim(0,2)\n",
    "\n",
    "\n",
    "    # plt.show()\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "    ax.set_axis_off()\n",
    "    plt.savefig(f'{img_name}.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LaTex for the rendering of the mathematical expression\n",
    "plt.rcParams.update({\n",
    "    'text.usetex': True,\n",
    "    \"font.family\": \"monospace\",\n",
    "    \"font.monospace\": 'Computer Modern Typewriter'\n",
    "})\n",
    "\n",
    "#Adjusting the resolution of the plot\n",
    "\n",
    "plt.figure(dpi=25)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "left, width = .25, .5\n",
    "bottom, height = .25, .5\n",
    "right = left + width\n",
    "top = bottom + height\n",
    "\n",
    "# Fractions and radicals\n",
    "ax.text(0.5 * (left + right), 0.5 * (bottom + top), r'$33x+222y$',\n",
    "        horizontalalignment='center',\n",
    "        verticalalignment='center',\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=150)\n",
    "\n",
    "# plt.text(1,1,r'$\\frac{x}{5}$',fontsize=200)\n",
    "\n",
    "#Adjusting the ticks size of the axes\n",
    "plt.xlim(0,2)\n",
    "plt.ylim(0,2)\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "plt.grid(False)\n",
    "plt.axis('off')\n",
    "ax.set_axis_off()\n",
    "plt.savefig('Teste.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Carregar a imagem\n",
    "imagem = cv2.imread('../data/Emc2.png')\n",
    "\n",
    "# Converter a imagem para escala de cinza\n",
    "imagem_cinza = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Aplicar limiariza√ß√£o na imagem para melhorar a qualidade do texto\n",
    "_, imagem_limiarizada = cv2.threshold(imagem_cinza, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Aplicar OCR para extrair o texto da imagem\n",
    "texto = pytesseract.image_to_string(imagem_limiarizada)\n",
    "\n",
    "# Remover espa√ßos em branco e quebras de linha\n",
    "texto = texto.replace(' ', '').replace('\\n', '')\n",
    "\n",
    "# Analisar a express√£o matem√°tica usando SymPy\n",
    "expr = sympy.sympify(texto)\n",
    "\n",
    "# Identificar a opera√ß√£o matem√°tica\n",
    "operacao = ''\n",
    "if isinstance(expr, sympy.Add):\n",
    "    operacao = 'adi√ß√£o'\n",
    "elif isinstance(expr, sympy.Mul):\n",
    "    operacao = 'multiplica√ß√£o'\n",
    "elif isinstance(expr, sympy.Pow):\n",
    "    operacao = 'potencia√ß√£o'\n",
    "elif isinstance(expr, sympy.Add):\n",
    "    operacao = 'adi√ß√£o'\n",
    "elif isinstance(expr, sympy.Add):\n",
    "    operacao = 'adi√ß√£o'\n",
    "elif isinstance(expr, sympy.Add):\n",
    "    operacao = 'adi√ß√£o'\n",
    "elif isinstance(expr, sympy.Add):\n",
    "    operacao = 'adi√ß√£o'\n",
    "elif isinstance(expr, sympy.Add):\n",
    "    operacao = 'adi√ß√£o'\n",
    "elif isinstance(expr, sympy.Add):\n",
    "    operacao = 'adi√ß√£o'\n",
    "\n",
    "# Gerar o c√≥digo LaTeX correspondente\n",
    "codigo_latex = sympy.latex(expr)\n",
    "\n",
    "# Imprimir a opera√ß√£o identificada e o c√≥digo LaTeX\n",
    "print(f\"A opera√ß√£o identificada √©: {operacao}\")\n",
    "print(f\"C√≥digo LaTeX: {codigo_latex}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml(\n",
    "    \"mnist_784\", version=1, return_X_y=True, as_frame=False, parser=\"pandas\"\n",
    ")\n",
    "X = X / 255.0\n",
    "\n",
    "# Split data into train partition and test partition\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.7)\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(40,),\n",
    "    max_iter=8,\n",
    "    alpha=1e-4,\n",
    "    solver=\"sgd\",\n",
    "    verbose=10,\n",
    "    random_state=1,\n",
    "    learning_rate_init=0.2,\n",
    ")\n",
    "\n",
    "# this example won't converge because of resource usage constraints on\n",
    "# our Continuous Integration infrastructure, so we catch the warning and\n",
    "# ignore it here\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
    "\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../im2Latex_Kaggle/im2latex_train.csv',sep=',',quotechar='\"')\n",
    "train = pd.read_csv(\"../im2Latex_Kaggle/im2latex_train.csv\", sep=\",\", quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2numpy(fname: str) -> np.ndarray:\n",
    "    fpath = rf\"../im2Latex_Kaggle/formula_images_processed/{fname}\"\n",
    "    img = cv2.imread(fpath, cv2.IMREAD_COLOR)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img_gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 0\n",
    "size = 0\n",
    "for img in train['image']:\n",
    "    if cont < 5000:\n",
    "        h,w = img2numpy(img)\n",
    "        img_size = h*w\n",
    "        if img_size > size:\n",
    "            size = img_size\n",
    "            print(img_size,h,w)\n",
    "        cont += 1\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt('../im2Latex_Kaggle/im2latex_train.csv',delimiter=',',dtype=str,quotechar='\"',usecols=(0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train[1:],columns=train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2numpy(fname:str) -> np.ndarray:\n",
    "    fpath = rf\"../im2Latex_Kaggle/formula_images_processed/{fname}\"\n",
    "    img = cv2.imread(fpath,cv2.IMREAD_COLOR)\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack([train[1],[img_gray.reshape(-1)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3], [4,5,6],[7,8,9]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.reshape(img,-1).shape)\n",
    "print(np.reshape(img_gray,-1).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_gray.reshape(-1).ndim)\n",
    "print(train[1].ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.column_stack([train[1],[img_gray]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train))\n",
    "print(type(img_gray))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VisComp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
